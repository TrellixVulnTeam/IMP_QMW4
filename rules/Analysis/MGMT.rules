rule ANALYSIS_ANNOTATE:
    log:
        AN_LOG
    benchmark:
        "%s/benchmarks/ANALYSIS_ANNOTATE.json" % AN_OUT
    input:
        '{dir}/MGMT.assembly.merged.fa'.format(dir=A_OUT),
        expand("{path}/{db}", path=DBPATH, db=config["prokka"]["databases"])
    output:
        "%s/annotation/annotation.filt.gff" % AN_OUT
    shell:
        """
        ### prokka by default will look databases where is located the binary.
        ### we have to softlink to put the binary somewhere and the databases somewhere else.
        if [[ "{DBPATH}" = /* ]]
        then
            PP={DBPATH};
        else
            PP=$PWD/{DBPATH};
        fi
        DD=$(dirname $(which prokka))/../db
        if [[ ! -L $DD ]]
        then
            CUR=$PWD
            echo "Softlinking $DD to $PP"
            cd $(dirname $(which prokka))/.. && ln -s $PP db
            cd $CUR
        fi

        prokka --force --outdir {AN_OUT}/annotation --cpus {THREADS} --metagenome --norrna {input[0]} >> {log} 2>&1

        # Prokka gives a weird gff file with all the sequences. We need to write some small code to produce a file that
        # cleans up the output

        LN=`grep -Hn "^>" {AN_OUT}/annotation/PROKKA_*.gff | head -n1 | cut -f2 -d ":"`
        LN1=1
        LN=$(($LN-$LN1))
        head -n $LN {AN_OUT}/annotation/*.gff | grep -v "^#" | sort | uniq | grep -v "^==" > {output}
        """

rule ANALYSIS_MG_CALL_GENE_DEPTH:
    log:
        AN_LOG
    benchmark:
        "%s/benchmarks/ANALYSIS_MG_CALL_GENE_DEPTH.json" % AN_OUT
    input:
        "%s/annotation/annotation.filt.gff" % AN_OUT,
        "%s/MG.reads.sorted.bam" % A_OUT
    output:
        "%s/MG.annotation.bed" % AN_OUT,
        "%s/MG.gene_depth.hist" % AN_OUT,
        "%s/MG.gene_depth.avg" % AN_OUT,
        "%s/MG.gene.len" % AN_OUT,
        "%s/MG.prokkaID2ec.txt" % AN_OUT
    shell:
        """
        coverageBed -hist -abam {input[1]} -b {input[0]} | grep -v "^all" > {output[0]}
        paste <(cat {output[0]} | cut -f9 | cut -f1 -d \";\" | sed -e \"s/ID=//g\") \
        <(cut -f10,11,12,13 {output[0]}) > {output[1]}
        ## This code was adapted and modified from the CONCOCT script to calculate depth
        ## It prints out a file that contains the average depth of all the genes
        awk -v OFS='\t' 'BEGIN {{pc=""}}
        {{
            c=$1;
            if (c == pc) {{
                    cov=cov+$2*$5;
            }} else {{
                print pc,cov;
                cov=$2*$5;
                pc=c
            }}
        }} END {{print pc,cov}}' < {output[1]} | tail -n +2 > {output[2]}
        # Record gene length file
        cut -f 1,4 {output[1]} | uniq > {output[3]}
        grep "eC_number=" {input[0]} | cut -f9 | cut -f1,2 -d ';'| sed 's/ID=//g'| sed 's/;eC_number=/\t/g' > {output[4]}
        """

rule ANALYSIS_MT_CALL_GENE_DEPTH:
    log:
        AN_LOG
    benchmark:
        "%s/benchmarks/ANALYSIS_MT_CALL_GENE_DEPTH.json" % AN_OUT
    input:
        "%s/annotation/annotation.filt.gff" % AN_OUT,
        "%s/MT.reads.sorted.bam" % A_OUT
    output:
        "%s/MT.annotation.bed" % AN_OUT,
        "%s/MT.gene_depth.hist" % AN_OUT,
        "%s/MT.gene_depth.avg" % AN_OUT,
        "%s/MT.gene.len" % AN_OUT,
        "%s/MT.prokkaID2ec.txt" % AN_OUT
    shell:
        """
        coverageBed -hist -abam {input[1]} -b {input[0]} | grep -v "^all" > {output[0]}
        paste <(cat {output[0]} | cut -f9 | cut -f1 -d \";\" | sed -e \"s/ID=//g\") \
        <(cut -f10,11,12,13 {output[0]}) > {output[1]}
        ## This code was adapted and modified from the CONCOCT script to calculate depth
        ## It prints out a file that contains the average depth of all the genes
        awk -v OFS='\t' 'BEGIN {{pc=""}}
        {{
            c=$1;
            if (c == pc) {{
                    cov=cov+$2*$5;
            }} else {{
                print pc,cov;
                cov=$2*$5;
                pc=c
            }}
        }} END {{print pc,cov}}' < {output[1]} | tail -n +2 > {output[2]}
        # Record gene length file
        cut -f 1,4 {output[1]} | uniq > {output[3]}
        grep "eC_number=" {input[0]} | cut -f9 | cut -f1,2 -d ';'| sed 's/ID=//g'| sed 's/;eC_number=/\t/g' > {output[4]}
        """

rule ANALYSIS_MG_CALL_VARIANT:
    log:
        AN_LOG
    benchmark:
        "%s/benchmarks/ANALYSIS_MG_CALL_VARIANT.json" % AN_OUT
    input:
        "%s/MGMT.assembly.merged.fa" % A_OUT,
        "%s/MG.reads.sorted.bam" % A_OUT,
    output:
        "%s/MG.variants.isec.vcf.gz" % AN_OUT,
        "%s/MG.variants.samtools.vcf.gz" % AN_OUT,
        "%s/MG.variants.freebayes.vcf.gz" % AN_OUT,
        "%s/MG.variants.platypus.vcf.gz" % AN_OUT
    shell:
        """
        echo "[x] MG VARIANT CALLING `date +"%Y/%m/%d %H:%M:%S"`"
        if [[ ! -f {input[1]}.bai ]]
        then
            echo "Bam index doesn't exist, Creating one..."
            echo "Indexing bam: {input[1]}"
            samtools index {input[1]}
        fi

        if [[ ! -f {input[0]}.fai ]]
        then
          echo "Fasta index doesn't exist, Creating one..."
          echo "Indexing fasta: {input[0]}"
          samtools faidx {input[0]}
        fi

        #temporary directory and files
        VCF_MPU=$(mktemp --tmpdir={TMPDIR} -t "XXXXXX.mpu.vcf")
        VCF_FRB=$(mktemp --tmpdir={TMPDIR} -t "XXXXXX.frb.vcf")
        VCF_PLT=$(mktemp --tmpdir={TMPDIR} -t "XXXXXX.plt.vcf")

        ### run_mpileup {input[0]} {input[1]} {output[1]}
        echo "Running samtools mpileup"
        samtools mpileup -uf {input[0]} {input[1]} |\
        bcftools view -vcg - |\
        vcf-convert -r {input[0]} -v 4.2 > $VCF_MPU
        bgzip -c $VCF_MPU > {output[1]}
        tabix -f -p vcf {output[1]}

        ### run_freebayes {input[0]} {input[1]} {output[2]}
        echo "Running freebayes"
        freebayes -f {input[0]} {input[1]} |\
        vcf-convert -r {input[0]} -v 4.2 > $VCF_FRB
        bgzip -c $VCF_FRB > {output[2]}
        tabix -f -p vcf {output[2]}

        ### run_platypus {input[0]} {input[1]} {output[3]}
        echo "Running platypus"
        Platypus.py callVariants --refFile={input[0]} \
        --bamFiles={input[1]} --nCPU={THREADS} -o $VCF_PLT
        bgzip -c $VCF_PLT > {output[3]}
        tabix -f -p vcf {output[3]}

        ### "Merging outputs from all the callers"
        ## Must remove colons from the contig names in upstream steps. Unable to merge the variants
        ## due to this problem
        vcf-isec -f -a -n +2 {output[1]} {output[2]} > {AN_OUT}/MG.variants.isec.vcf

        # Compress and index the output.
        bgzip -c {AN_OUT}/MG.variants.isec.vcf > {output[0]}
        tabix -f -p vcf {output[0]}

        # Clean up directory
        echo "Cleaning up directory"
        cat log.txt >> {log}
        rm -f {AN_OUT}/MG.variants.isec.vcf log.txt
        """

rule ANALYSIS_MT_CALL_VARIANT:
    log:
        AN_LOG
    benchmark:
        "%s/benchmarks/ANALYSIS_MT_CALL_VARIANT.json" % AN_OUT
    input:
        "%s/MGMT.assembly.merged.fa" % A_OUT,
        "%s/MT.reads.sorted.bam" % A_OUT,
    output:
        "%s/MT.variants.isec.vcf.gz" % AN_OUT,
        "%s/MT.variants.samtools.vcf.gz" % AN_OUT,
        "%s/MT.variants.freebayes.vcf.gz" % AN_OUT,
        "%s/MT.variants.platypus.vcf.gz" % AN_OUT
    shell:
        """
        echo "[x] MT VARIANT CALLING `date +"%Y/%m/%d %H:%M:%S"`"
        if [[ ! -f {input[1]}.bai ]]
        then
            echo "Bam index doesn't exist, Creating one..."
            echo "Indexing bam: {input[1]}"
            samtools index {input[1]}
        fi

        if [[ ! -f {input[0]}.fai ]]
        then
          echo "Fasta index doesn't exist, Creating one..."
          echo "Indexing fasta: {input[0]}"
          samtools faidx {input[0]}
        fi

        #temporary directory and files
        VCF_MPU=$(mktemp --tmpdir={TMPDIR} -t "XXXXXX.mpu.vcf")
        VCF_FRB=$(mktemp --tmpdir={TMPDIR} -t "XXXXXX.frb.vcf")
        VCF_PLT=$(mktemp --tmpdir={TMPDIR} -t "XXXXXX.plt.vcf")

        ### run_mpileup {input[0]} {input[1]} {output[1]}
        echo "Running samtools mpileup"
        samtools mpileup -uf {input[0]} {input[1]} |\
        bcftools view -vcg - |\
        vcf-convert -r {input[0]} -v 4.2 > $VCF_MPU
        bgzip -c $VCF_MPU > {output[1]}
        tabix -f -p vcf {output[1]}

        ### run_freebayes {input[0]} {input[1]} {output[2]}
        echo "Running freebayes"
        freebayes -f {input[0]} {input[1]} |\
        vcf-convert -r {input[0]} -v 4.2 > $VCF_FRB
        bgzip -c $VCF_FRB > {output[2]}
        tabix -f -p vcf {output[2]}

        ### run_platypus {input[0]} {input[1]} {output[3]}
        echo "Running platypus"
        Platypus.py callVariants --refFile={input[0]} \
        --bamFiles={input[1]} --nCPU={THREADS} -o $VCF_PLT
        bgzip -c $VCF_PLT > {output[3]}
        tabix -f -p vcf {output[3]}

        ### "Merging outputs from all the callers"
        ## Must remove colons from the contig names in upstream steps. Unable to merge the variants
        ## due to this problem
        vcf-isec -f -a -n +2 {output[1]} {output[2]} > {AN_OUT}/MT.variants.isec.vcf

        # Compress and index the output.
        bgzip -c {AN_OUT}/MT.variants.isec.vcf > {output[0]}
        tabix -f -p vcf {output[0]}

        # Clean up directory
        echo "Cleaning up directory"
        cat log.txt >> {log}
        rm -f {AN_OUT}/MT.variants.isec.vcf log.txt
        """

rule ANALYSIS_HYBRID_CONTIG_LENGTH:
    log:
        AN_LOG
    benchmark:
        "%s/benchmarks/ANALYSIS_MGMT_CONTIG_LENGTH.json" % AN_OUT
    input:
        "%s/MGMT.assembly.merged.fa" % A_OUT,
    output:
        "%s/MGMT.assembly.length.txt" % AN_OUT,
        "%s/MGMT.assembly.gc_content.txt" % AN_OUT,
    shell:
        """
        echo "[x]  LENGTH `date +"%Y/%m/%d %H:%M:%S"`" >> {log}
        echo "Obtaining contig lengths"
        perl {SRCDIR}/fastaNamesSizes.pl {input} > {output[0]}

        echo "Obtaining GC content"
        TMP_GC=$(mktemp --tmpdir={TMPDIR} -t "gc_out_XXXXXX.txt")
        perl {SRCDIR}/get_GC_content.pl {input} $TMP_GC

        # Th program above provides a file gc_out.txt. This command cleans the output
        echo "Clean up output"
        cut -f1,2 $TMP_GC | sed -e 's/>//g'> {output[1]}
        echo "Remove intermediate files"
        rm $TMP_GC
        """

rule ANALYSIS_MG_CALL_CONTIG_DEPTH:
    log:
        AN_LOG
    benchmark:
        "%s/benchmarks/ANALYSIS_MG_CALL_CONTIG_DEPTH.json" % AN_OUT
    input:
        "%s/MG.reads.sorted.bam" % A_OUT,
        "%s/MGMT.assembly.merged.fa" % A_OUT,
    output:
        "%s/MG.assembly.contig_coverage.txt" % AN_OUT,
        "%s/MG.assembly.contig_depth.txt" % AN_OUT,
        "%s/MG.assembly.contig_flagstat.txt" % AN_OUT,
    shell:
        """
        echo "[x]  COVERAGE AND DEPTH `date +"%Y/%m/%d %H:%M:%S"`" >> {log}
        echo "Creating genome file ..." >> {log}
        if [[ ! -f {input[1]}.fai ]]
        then
          echo "No fasta index! Creating one." >> {log}
          samtools faidx {input[1]}
        fi
        cat {input[1]}.fai | awk '{{print $1 \"\t0\t\" $2}}' > {input[1]}.bed3
        echo "Done creating bed file" >> {log}

        echo "Running BEDTools coverage calculation ..." >> {log}
        coverageBed -abam {input[0]} -b {input[1]}.bed3 > {output[0]}
        echo "Coverage calculation done" >> {log}
        echo "Running BEDTools for average depth in each position" >> {log}
        TMP_DEPTH=$(mktemp --tmpdir={TMPDIR} -t "depth_file_XXXXXX.txt")
        genomeCoverageBed -ibam {input[0]} | grep -v "genome" > $TMP_DEPTH
        echo "Depth calculation done" >> {log}

        ## This method of depth calculation was adapted and modified from the CONCOCT code
        awk -v OFS='\t' 'BEGIN {{pc=""}}
        {{
        c=$1;
        if (c == pc) {{
        cov=cov+$2*$5;
        }} else {{
        print pc,cov;
        cov=$2*$5;
        pc=c}}
        }} END {{print pc,cov}}' $TMP_DEPTH | tail -n +2 > {output[1]}

        echo "Remove the temporary file" >> {log}
        rm $TMP_DEPTH
        echo "flagstat" >> {log}
        samtools flagstat {input[0]} | cut -f1 -d ' ' > {output[2]}
        """

rule ANALYSIS_MT_CALL_CONTIG_DEPTH:
    log:
        AN_LOG
    benchmark:
        "%s/benchmarks/ANALYSIS_MT_CALL_CONTIG_DEPTH.json" % AN_OUT
    input:
        "%s/MT.reads.sorted.bam" % A_OUT,
        "%s/MGMT.assembly.merged.fa" % A_OUT,
    output:
        "%s/MT.assembly.contig_coverage.txt" % AN_OUT,
        "%s/MT.assembly.contig_depth.txt" % AN_OUT,
        "%s/MT.assembly.contig_flagstat.txt" % AN_OUT,
    shell:
        """
        echo "[x]  COVERAGE AND DEPTH `date +"%Y/%m/%d %H:%M:%S"`" >> {log}
        echo "Creating genome file ..." >> {log}
        if [[ ! -f {input[1]}.fai ]]
        then
          echo "No fasta index! Creating one." >> {log}
          samtools faidx {input[1]}
        fi
        cat {input[1]}.fai | awk '{{print $1 \"\t0\t\" $2}}' > {input[1]}.bed3
        echo "Done creating bed file" >> {log}

        echo "Running BEDTools coverage calculation ..." >> {log}
        coverageBed -abam {input[0]} -b {input[1]}.bed3 > {output[0]}
        echo "Coverage calculation done" >> {log}
        echo "Running BEDTools for average depth in each position" >> {log}
        TMP_DEPTH=$(mktemp --tmpdir={TMPDIR} -t "depth_file_XXXXXX.txt")
        genomeCoverageBed -ibam {input[0]} | grep -v "genome" > $TMP_DEPTH
        echo "Depth calculation done" >> {log}

        ## This method of depth calculation was adapted and modified from the CONCOCT code
        awk -v OFS='\t' 'BEGIN {{pc=""}}
        {{
        c=$1;
        if (c == pc) {{
        cov=cov+$2*$5;
        }} else {{
        print pc,cov;
        cov=$2*$5;
        pc=c}}
        }} END {{print pc,cov}}' $TMP_DEPTH | tail -n +2 > {output[1]}

        echo "Remove the temporary file" >> {log}
        rm $TMP_DEPTH
        echo "flagstat" >> {log}
        samtools flagstat {input[0]} | cut -f1 -d ' ' > {output[2]}
        """

def mg_fqfiles():
    raw = expand('{dir}/{raw}', raw=['MG.R1.fq', 'MG.R2.fq'], dir=P_OUT)
    trim = expand('{dir}/{trim}', trim=[
        'MG.R1.trimmed.fq',
        'MG.R2.trimmed.fq',
        'MG.SE.trimmed.fq'], dir=P_OUT)
    filter = expand('{dir}/{filter}', filter=expand([
        'MG.R1.trimmed.{f}.fq',
        'MG.R2.trimmed.{f}.fq',
        'MG.SE.trimmed.{f}.fq'], f=config['human_filtering']['filter']), dir=P_OUT)
    if config['preprocessing_filtering']:
        return raw + trim + filter
    return raw + trim

def mt_fqfiles():
    raw = expand('{dir}/{raw}', raw=['MT.R1.fq', 'MT.R2.fq'], dir=P_OUT)
    trim = expand('{dir}/{trim}', trim=[
        'MT.R1.trimmed.fq',
        'MT.R2.trimmed.fq',
        'MT.SE.trimmed.fq'], dir=P_OUT)
    rna_filter = expand('{dir}/{filter}', filter=[
        'MT.R1.trimmed.rna_filtered.fq',
        'MT.R2.trimmed.rna_filtered.fq',
        'MT.SE.trimmed.rna_filtered.fq'], dir=P_OUT)
    h_filter = expand('{dir}/{filter}', filter=expand([
        'MT.R1.trimmed.rna_filtered.{f}.fq',
        'MT.R2.trimmed.rna_filtered.{f}.fq',
        'MT.SE.trimmed.rna_filtered.{f}.fq'], f=config['human_filtering']['filter']), dir=P_OUT)
    if config['preprocessing_filtering']:
        return raw + trim + rna_filter + h_filter
    return raw + trim + rna_filter

rule ANALYSIS_MG_READ_COUNT:
    log:
        AN_LOG
    benchmark:
        "%s/benchmarks/ANALYSIS_MG_READ_COUNT.json" % AN_OUT
    input:
        mg_fqfiles()
    output:
        '%s/MG.read_counts.txt' % AN_OUT
    run:
        for idx, f in enumerate({input}):
            if idx == 0:
                shell("wc -l {f} > {output}")
            else:
                shell("wc -l {f} >> {output}")

rule ANALYSIS_MT_READ_COUNT:
    log:
        AN_LOG
    benchmark:
        "%s/benchmarks/ANALYSIS_MT_READ_COUNT.json" % AN_OUT
    input:
        mt_fqfiles()
    output:
        '%s/MT.read_counts.txt' % AN_OUT
    run:
        for idx, f in enumerate({input}):
            if idx == 0:
                shell("wc -l {f} > {output}")
            else:
                shell("wc -l {f} >> {output}")

rule ANALYSIS_VIZBIN:
    log:
        AN_LOG
    benchmark:
        "%s/benchmarks/ANALYSIS_VIZBIN.json" % AN_OUT
    input:
        "%s/MGMT.assembly.merged.fa" % A_OUT,
    output:
        "%s/MGMT.vizbin.filtered.fa" % AN_OUT,
        "%s/MGMT.vizbin.with-contig-names.points" % AN_OUT
    shell:
        """
        echo "[x] VIZBIN `date +"%Y/%m/%d %H:%M:%S"`" >> {log}
        TMP_VIZBIN=$(mktemp --tmpdir={TMPDIR} -dt "VIZBIN_XXXXXX")
        perl src/fasta_filter_length.pl 1000 {input[0]} > {output[0]}

        java -jar {config[vizbin][jarfile]} \
        -a {config[vizbin][dimension]} \
        -c {config[vizbin][cutoff]} \
        -i {output[0]} \
        -o $TMP_VIZBIN/data.points \
        -k {config[vizbin][kmer]} \
        -p {config[vizbin][perp]} >> {log}

        if [ -f $TMP_VIZBIN/data.points ]
            then
                paste <(grep "^>" {output[0]} | sed -e 's/>//') \
                <(cat $TMP_VIZBIN/data.points | sed -e 's/,/\t/') > {output[1]}
            fi
        rm -rf $TMP_VIZBIN
        """

rule ANALYSIS_VISUALIZE:
    log:
        AN_LOG
    benchmark:
        "%s/benchmarks/ANALYSIS_VISUALIZE.json" % AN_OUT
    input:
        expand('{dir}/{name}', name=[
               'MG.read_counts.txt',
               'MT.read_counts.txt',
               'MG.assembly.contig_flagstat.txt',
               'MT.assembly.contig_flagstat.txt',
               'MG.assembly.contig_coverage.txt',
               'MT.assembly.contig_coverage.txt',
               'MG.assembly.contig_depth.txt',
               'MT.assembly.contig_depth.txt',
               'MG.variants.samtools.vcf.gz',
               'MT.variants.samtools.vcf.gz',
               'MGMT.assembly.gc_content.txt',
               "MGMT.vizbin.with-contig-names.points",
               "annotation/annotation.filt.gff",
               "results/quast/summary/combined_quast_output/contigs_reports/nucmer_output/aux/MGMT.assembly.merged.coords_edited"], dir=AN_OUT)
    output:
        analysis_plot_files_output()
    params:
        outdir = "%s/results" % AN_OUT
    shell:
        """
        PLOT_SCRIPT="{SRCDIR}/IMP_visualize_MGMT.R"

        echo "[x] PLOT `date +"%Y/%m/%d %H:%M:%S"`" >> {log}
        mkdir -p {AN_OUT}/results
        Rscript $PLOT_SCRIPT {AN_OUT}/results {input}
        """

rule ANALYSIS_MG_KRONA:
    log:
        AN_LOG
    benchmark:
        "%s/benchmarks/ANALYSIS_MG_KRONA.json" % AN_OUT
    input:
        "%s/MG.prokkaID2ec.txt" % AN_OUT,
        "%s/ec2pathway.txt" % DBPATH,
        "%s/pathway2hierarchy.txt" % DBPATH,
        "%s/MG.gene_depth.avg" % AN_OUT,
        "%s/MG.gene.len" % AN_OUT
    output:
        "%s/results/MG.gene_kegg_krona.txt" % AN_OUT,
        "%s/results/MG.gene_kegg_krona.html" % AN_OUT
    shell:
        """
        echo "[x] PLOT KRONA `date +"%Y/%m/%d %H:%M:%S"`" >> {log}
        echo {input}
        echo {output[0]}
        python {SRCDIR}/genes.to.kronaTable.py -i {input[0]} -m {input[1]} -H {input[2]} -c {input[3]} -L {input[4]} -o {output[0]}
        ktImportText -o {output[1]} {output[0]} -a
        """

rule ANALYSIS_MT_KRONA:
    log:
        AN_LOG
    benchmark:
        "%s/benchmarks/ANALYSIS_MT_KRONA.json" % AN_OUT
    input:
        "%s/MT.prokkaID2ec.txt" % AN_OUT,
        "%s/ec2pathway.txt" % DBPATH,
        "%s/pathway2hierarchy.txt" % DBPATH,
        "%s/MT.gene_depth.avg" % AN_OUT,
        "%s/MT.gene.len" % AN_OUT
    output:
        "%s/results/MT.gene_kegg_krona.txt" % AN_OUT,
        "%s/results/MT.gene_kegg_krona.html" % AN_OUT
    shell:
        """
        echo "[x] PLOT KRONA `date +"%Y/%m/%d %H:%M:%S"`" >> {log}
        echo {input}
        echo {output[0]}
        python {SRCDIR}/genes.to.kronaTable.py -i {input[0]} -m {input[1]} -H {input[2]} -c {input[3]} -L {input[4]} -o {output[0]}
        ktImportText -o {output[1]} {output[0]} -a
        """

rule ANALYSIS_MG_FASTQC:
    input:
        expand('{dir}/{raw}', raw=['MG.R1.fq', 'MG.R2.fq'], dir=P_OUT)
    output:
        expand('{dir}/stats/MG/MG.R1.fq_fastqc.html', dir=AN_OUT),
        expand('{dir}/stats/MG/MG.R2.fq_fastqc.html', dir=AN_OUT),
        expand('{dir}/stats/MG/MG.R1.fq_fastqc.zip', dir=AN_OUT),
        expand('{dir}/stats/MG/MG.R2.fq_fastqc.zip', dir=AN_OUT)
    benchmark:
        "%s/benchmarks/PREPROCESSING_MG_FASTQC.json" % AN_OUT
    log:
        AN_LOG
    shell:
        """
        mkdir -p {AN_OUT}/stats/MG
        fastqc -o {AN_OUT}/stats/MG -f fastq {input[0]} {input[1]} -t {THREADS} -d {TMPDIR}
        """

rule ANALYSIS_MG_PREPROCESSED_FASTQC:
    input:
        preprocessed_mg('R1'),
        preprocessed_mg('R2'),
        preprocessed_mg('SE')
    output:
        preprocessed_mg('R1', dir=AN_OUT + '/stats/MG') + '_fastqc.zip',
        preprocessed_mg('R2', dir=AN_OUT + '/stats/MG') + '_fastqc.zip',
        preprocessed_mg('SE', dir=AN_OUT + '/stats/MG') + '_fastqc.zip',
        preprocessed_mg('R1', dir=AN_OUT + '/stats/MG') + '_fastqc.html',
        preprocessed_mg('R2', dir=AN_OUT + '/stats/MG') + '_fastqc.html',
        preprocessed_mg('SE', dir=AN_OUT + '/stats/MG') + '_fastqc.html'
    benchmark:
        "%s/benchmarks/PREPROCESSING_MG_FASTQC.json" % AN_OUT
    log:
        AN_LOG
    shell:
        """
        mkdir -p {AN_OUT}/stats/MG
        if [[ -s {input[2]} ]]
        then
        fastqc -o {AN_OUT}/stats/MG -f fastq {input} -t {THREADS} -d {TMPDIR}
        else
        echo "{input[2]} is empty, FastQC will not run for that file"
        fastqc -o {AN_OUT}/stats/MG -f fastq {input[0]} {input[1]} -t {THREADS} -d {TMPDIR}
        echo "Creating empty files for {input[2]}..."
        touch {output[2]}
        echo "No single end reads were generated" > {output[5]}
        fi
        """

rule RENAME_STAT_OUTPUT:
    input:
        preprocessed_mt('R1', dir=AN_OUT + '/stats/MT') + '_fastqc.zip',
        preprocessed_mt('R2', dir=AN_OUT + '/stats/MT') + '_fastqc.zip',
        preprocessed_mt('SE', dir=AN_OUT + '/stats/MT') + '_fastqc.zip',
        preprocessed_mt('R1', dir=AN_OUT + '/stats/MT') + '_fastqc.html',
        preprocessed_mt('R2', dir=AN_OUT + '/stats/MT') + '_fastqc.html',
        preprocessed_mt('SE', dir=AN_OUT + '/stats/MT') + '_fastqc.html',
        preprocessed_mg('R1', dir=AN_OUT + '/stats/MG') + '_fastqc.zip',
        preprocessed_mg('R2', dir=AN_OUT + '/stats/MG') + '_fastqc.zip',
        preprocessed_mg('SE', dir=AN_OUT + '/stats/MG') + '_fastqc.zip',
        preprocessed_mg('R1', dir=AN_OUT + '/stats/MG') + '_fastqc.html',
        preprocessed_mg('R2', dir=AN_OUT + '/stats/MG') + '_fastqc.html',
        preprocessed_mg('SE', dir=AN_OUT + '/stats/MG') + '_fastqc.html'
    output:
        "%s/stats/MT/MT.R1_preprocessed_fastqc.zip" % AN_OUT,
        "%s/stats/MT/MT.R2_preprocessed_fastqc.zip" % AN_OUT,
        "%s/stats/MT/MT.SE_preprocessed_fastqc.zip" % AN_OUT,
        "%s/stats/MT/MT.R1_preprocessed_fastqc.html" % AN_OUT,
        "%s/stats/MT/MT.R2_preprocessed_fastqc.html" % AN_OUT,
        "%s/stats/MT/MT.SE_preprocessed_fastqc.html" % AN_OUT,
        "%s/stats/MG/MG.R1_preprocessed_fastqc.zip" % AN_OUT,
        "%s/stats/MG/MG.R2_preprocessed_fastqc.zip" % AN_OUT,
        "%s/stats/MG/MG.SE_preprocessed_fastqc.zip" % AN_OUT,
        "%s/stats/MG/MG.R1_preprocessed_fastqc.html" % AN_OUT,
        "%s/stats/MG/MG.R2_preprocessed_fastqc.html" % AN_OUT,
        "%s/stats/MG/MG.SE_preprocessed_fastqc.html" % AN_OUT
    run:
        for index, _file in enumerate(input):
            shell("mv %s %s" % (_file, output[index]))

rule ANALYSIS_MT_FASTQC:
    input:
        expand('{dir}/{raw}', raw=['MT.R1.fq', 'MT.R2.fq'], dir=P_OUT)
    output:
        expand('{dir}/stats/MT/MT.R1.fq_fastqc.html', dir=AN_OUT),
        expand('{dir}/stats/MT/MT.R2.fq_fastqc.html', dir=AN_OUT),
        expand('{dir}/stats/MT/MT.R1.fq_fastqc.zip', dir=AN_OUT),
        expand('{dir}/stats/MT/MT.R2.fq_fastqc.zip', dir=AN_OUT)
    benchmark:
        "%s/benchmarks/PREPROCESSING_MT_FASTQC.json" % AN_OUT
    log:
        AN_LOG
    shell:
        """
        mkdir -p {AN_OUT}/stats/MT
        fastqc -o {AN_OUT}/stats/MT -f fastq {input[0]} {input[1]} -t {THREADS} -d {TMPDIR}
        """

rule ANALYSIS_MT_PREPROCESSED_FASTQC:
    input:
        preprocessed_mt('R1'),
        preprocessed_mt('R2'),
        preprocessed_mt('SE')
    output:
        preprocessed_mt('R1', dir=AN_OUT + '/stats/MT') + '_fastqc.zip',
        preprocessed_mt('R2', dir=AN_OUT + '/stats/MT') + '_fastqc.zip',
        preprocessed_mt('SE', dir=AN_OUT + '/stats/MT') + '_fastqc.zip',
        preprocessed_mt('R1', dir=AN_OUT + '/stats/MT') + '_fastqc.html',
        preprocessed_mt('R2', dir=AN_OUT + '/stats/MT') + '_fastqc.html',
        preprocessed_mt('SE', dir=AN_OUT + '/stats/MT') + '_fastqc.html'
    benchmark:
        "%s/benchmarks/PREPROCESSING_MT_FASTQC.json" % AN_OUT
    log:
        AN_LOG
    shell:
        """
        mkdir -p {AN_OUT}/stats/MT
        if [[ -s {input[2]} ]]
        then
        fastqc -o {AN_OUT}/stats/MT -f fastq {input} -t {THREADS} -d {TMPDIR}
        else
        echo "{input[2]} is empty, FastQC will not run for that file"
        fastqc -o {AN_OUT}/stats/MT -f fastq {input[0]} {input[1]} -t {THREADS} -d {TMPDIR}
        echo "Creating empty files for {input[2]}..."
        touch {output[2]}
        echo "No single end reads were generated" > {output[5]}
        fi
        """

rule ANALYSIS_QUAST:
    input:
        '{dir}/MGMT.assembly.merged.fa'.format(dir=A_OUT)
    output:
        "%s/results/quast/summary/report.html" % AN_OUT,
        "%s/results/quast/summary/combined_quast_output/contigs_reports/nucmer_output/aux/MGMT.assembly.merged.coords_edited" % AN_OUT
    benchmark:
        "%s/benchmarks/ANALYSIS_QUAST.json" % AN_OUT
    log:
        AN_LOG
    shell:
        """
        metaquast -t {THREADS} -o {AN_OUT}/results/quast {input[0]} --max-ref-number 75
        sed -e 's/|//g' {AN_OUT}/results/quast/combined_quast_output/contigs_reports/nucmer_output/aux/MGMT.assembly.merged.coords \
        | sed -e 's/*\s/\t/g' | tail -n +3 > {output[1]}
        """
